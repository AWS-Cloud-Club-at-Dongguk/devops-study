# 📚 웹 크롤러 설계 — Study Notes

## 1. 웹 크롤러란?

웹 페이지를 자동으로 탐색하여 데이터를 수집하는 시스템.

### 주요 사용 사례

- 검색 엔진 인덱싱 (Googlebot 등)
- 데이터 마이닝
- 웹 아카이빙
- 가격 비교 서비스
- AI 학습 데이터 수집

---

## 2. 설계 목표 (Requirements)

좋은 크롤러가 갖춰야 할 핵심 특성:

- Scalability (확장성)
- Politeness (예의)
- Freshness (신선도)
- Efficiency (효율성)
- Robustness (안정성)
- Extensibility (확장 가능성)

---

## 3. 웹 구조 모델

웹은 **유향 그래프 (Directed Graph)** 로 표현 가능.

- 노드 = 웹 페이지
- 엣지 = 하이퍼링크(URL)

---

## 4. 크롤링 전략

### 4.1 BFS (Breadth-First Search)

기본 탐색 방식.

#### 장점
- 넓게 탐색 → 특정 영역에 갇히지 않음
- 중요 페이지 발견 확률 높음

#### DFS가 부적합한 이유
- 무한 깊이 탐색 위험
- Spider Trap에 취약

---

## 5. URL Frontier

크롤링 대기 중인 URL 집합.

핵심 역할:

- 다음에 방문할 URL 관리
- 우선순위 관리
- 예의 정책 적용
- 재크롤링 관리

---

## 6. Politeness (예의)

특정 사이트에 과도한 요청을 보내면 안 됨.

### 문제

한 페이지의 링크 대부분은 같은 호스트 → 요청 집중 발생.

### 해결: Host 기반 큐 분리

example.com → Queue A  
google.com  → Queue B  

#### 구성 요소

- Queue Router → 호스트별 큐로 분배
- Mapping Table → host → queue
- FIFO Queue → URL 저장
- Queue Selector → 어떤 큐에서 가져올지 결정
- Worker → 다운로드 수행

👉 동일 호스트 요청 간 일정 시간 간격 유지

---

## 7. 우선순위 기반 크롤링

모든 페이지가 동일하게 중요하지 않음.

### 중요도 판단 기준

- PageRank
- 트래픽
- 업데이트 빈도
- 사용자 관심도
- 사이트 신뢰도

---

## 8. Front Queue / Back Queue 구조

대규모 크롤러에서 사용되는 핵심 구조.

### Front Queue
→ 중요도 기반 정렬

### Back Queue
→ 호스트별 politeness 보장

URL  
 ↓  
Prioritizer  
 ↓  
Front Queues (importance-based)  
 ↓  
Queue Selector  
 ↓  
Back Queues (host-based)  
 ↓  
Workers  

---

## 9. Freshness (재크롤링)

웹 페이지는 지속적으로 변경됨.

### 전략

- 변경 이력 기반
- 중요 페이지 자주 방문
- 비용 대비 효율 고려

---

## 10. URL 저장 전략

수십억 개 URL 처리 필요.

### Hybrid 저장 방식

- 대부분 → 디스크
- 일부 → 메모리 버퍼
- 주기적 flush

---

## 11. Robots.txt 처리

사이트 크롤링 규칙 준수 필수.

### 기능

- 허용/금지 경로 확인
- 주기적 재다운로드
- 캐싱

---

## 12. 성능 최적화

### 12.1 분산 크롤링

- 여러 서버 사용
- URL 공간 분할
- 병렬 처리

### 12.2 DNS 캐싱

DNS lookup은 느림 → 캐시 필요

### 12.3 지역성(Locality)

대상 서버와 가까운 위치에서 크롤링

### 12.4 타임아웃 설정

느린 서버로 인한 병목 방지

---

## 13. 안정성 설계

### 주요 기법

- Consistent Hashing → 서버 변화 대응
- 상태 저장 (checkpoint)
- 장애 복구 가능
- 예외 처리
- 데이터 검증

---

## 14. 확장성

플러그인 구조 지원:

- 이미지 다운로드
- 콘텐츠 필터
- 모니터링
- 새로운 데이터 타입 처리

---

## 15. 문제 콘텐츠 처리

### 15.1 중복 콘텐츠

해결 방법:

- 해시 비교
- 체크섬
- Bloom filter

---

### 15.2 Spider Trap

무한 링크 구조.

예:

calendar?page=1  
calendar?page=2  
...  

해결:

- URL 길이 제한
- 패턴 감지
- 깊이 제한

---

### 15.3 스팸 및 품질 낮은 페이지

필터링 필요:

- 광고 페이지
- 자동 생성 콘텐츠
- 악성 사이트

---

## 16. 서버 측 렌더링 문제

많은 사이트가 JavaScript 기반.

### 문제

정적 HTML만 수집하면 동적 콘텐츠 미수집.

### 해결 방법

- Server-Side Rendering
- Dynamic Rendering
- Headless Browser 사용

---

## 17. 저장 시스템 설계

### DB 복제 및 샤딩

목적:

- 가용성 향상
- 확장성 확보
- 성능 개선

---

## 18. 대규모 확장 (Horizontal Scaling)

대규모 크롤링 → 수백~수천 서버 필요.

### 핵심 조건

👉 Stateless 구조

- 서버가 상태를 저장하지 않음
- 장애 시 쉽게 교체 가능

---

## 19. 시스템 전반 고려사항

### 필수 시스템 속성

- Availability (가용성)
- Consistency (일관성)
- Reliability (신뢰성)

---

## 20. 데이터 분석 (Analytics)

크롤링 데이터를 활용하여:

- 시스템 성능 튜닝
- 우선순위 조정
- 품질 개선

---

## 21. 핵심 설계 철학 (Interview Key)

좋은 크롤러는 단순히 빠른 시스템이 아니다.

### 균형 잡힌 시스템

- Scalability
- Politeness
- Freshness
- Priority
- Fault tolerance
- Efficiency
- Extensibility

---

## 🧠 한 줄 핵심 요약

웹 크롤러는 “대규모 분산 탐색 시스템 + 예의 정책 + 데이터 수집 플랫폼”이다.